#单线程爬取详情页
import time
t1=time.time()
import requests
def getHTMLText(url):
    try:      
        r=requests.get(url,timeout=30)
        r.raise_for_status()
        r.encoding='utf-8'
        return r.json()
    except:
        pass
for i in range(1,101):
    url=f"https://spa1.scrape.center/api/movie/{i}/"
    html=getHTMLText(url)
    print(i,html['drama'])
print(time.time()-t1)#约一分钟
*******************************************************
#异步爬取详情页
import time
from requests.exceptions import Timeout
t1=time.time()
import requests
from lxml import etree
#异步爬取详情页
import asyncio
import aiohttp
template = 'https://spa1.scrape.center/api/movie/{page}'
async def get(session, queue):
    while True:
        try:
            page = queue.get_nowait()
        except asyncio.QueueEmpty:
            return        
        try:
            url = template.format(page=page)
            resp = await session.get(url,timeout=120)
            r=await resp.json(encoding='utf-8')
            r=r['drama']
            print(page,r)
        except:
            print('error:',page)        
async def main():
    async with aiohttp.ClientSession() as session:
        queue = asyncio.Queue()
        for page in range(1,101):
            queue.put_nowait(page)
        tasks = []
        for _ in range(100):
            task = get(session, queue)
            tasks.append(task)
        await asyncio.wait(tasks)
loop = asyncio.get_event_loop()
loop.run_until_complete(main())
print(time.time()-t1)#约6秒
